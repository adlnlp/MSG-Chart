{"cells":[{"cell_type":"markdown","metadata":{"id":"VbRM30A7PrSa"},"source":["#Downloads"]},{"cell_type":"code","source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"metadata":{"id":"VUPpqVTCd2c8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_id = '1-F07uiQtPtlISfml0Y_0xFxgirEmiXuq' # URL id.\n","downloaded = drive.CreateFile({'id': file_id})\n","downloaded.GetContentFile('mask-rcnn-predict_pkl.zip')"],"metadata":{"id":"8NNoC1pCyxR6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_id = '13Yq7zielAiyBZJu6OjIrj8orlPTQ0OQd' # URL id.\n","downloaded = drive.CreateFile({'id': file_id})\n","downloaded.GetContentFile('text_feature_bert_ltfeat.zip')"],"metadata":{"id":"CpYhr6rvzPVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_id = '1wSLMZ-Qjoe6EoxYvW_4GBwwaXCE9xvf6' # URL id.\n","downloaded = drive.CreateFile({'id': file_id})\n","downloaded.GetContentFile('OpenCQA_Graph.zip')"],"metadata":{"id":"CdQvUB0Czc3H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_id = '1Zo7t0j2jZ2jzDa5Y0h7cCq5D6YVGKrKx' # URL id.\n","downloaded = drive.CreateFile({'id': file_id})\n","downloaded.GetContentFile('OpenCQA_Graph_6_rels_pie.zip')"],"metadata":{"id":"0zD3qMzwzl67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_id = '1xYcRy1EMQF1Iyj0ZbIyEc-2b377n0KP4' # URL id.\n","downloaded = drive.CreateFile({'id': file_id})\n","downloaded.GetContentFile('unichart_patch_object_pred_ae.zip')"],"metadata":{"id":"KQw8zXOYzvhk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_id = '1_15A3I1he-SH_yS0vkZInDDEZakz-VLb' # URL id.\n","downloaded = drive.CreateFile({'id': file_id})\n","downloaded.GetContentFile('unichart_patch_len_pred_ae.zip')"],"metadata":{"id":"Eo0FHnhkz38B"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-OB55aY-PrSc"},"outputs":[],"source":["!unzip mask-rcnn-predict_pkl.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VMBc7Hi4ckjI"},"outputs":[],"source":["!unzip text_feature_bert_ltfeat.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SkSr0sstc0Qx"},"outputs":[],"source":["!unzip OpenCQA_Graph.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vKGBl70fc62j"},"outputs":[],"source":["!unzip OpenCQA_Graph_6_rels_pie.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xt1l6wCIPniK"},"outputs":[],"source":["!unzip unichart_patch_object_pred_ae.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nB6tIcStQ1TR"},"outputs":[],"source":["!unzip unichart_patch_len_pred_ae.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HCvbokhfPrSc"},"outputs":[],"source":["!git clone https://github.com/vis-nlp/OpenCQA.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HCXJO0jSPrSc"},"outputs":[],"source":["!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3xr3aPuPrSc"},"outputs":[],"source":["!pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eX4jF7xWPrSd"},"outputs":[],"source":["!pip install sacrebleu\n","!pip install sacremoses"]},{"cell_type":"markdown","metadata":{"id":"cGgO6hTD3OUO"},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5qbRney3OUW"},"outputs":[],"source":["import torch\n","torch.manual_seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1jJh8dNO3OUW"},"outputs":[],"source":["import os\n","\n","def create_folder_if_not_exists(folder_path):\n","    if not os.path.exists(folder_path):\n","        os.makedirs(folder_path)\n","        print(f\"Folder '{folder_path}' created successfully.\")\n","    else:\n","        print(f\"Folder '{folder_path}' already exists.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQIwkTmj3OUW"},"outputs":[],"source":["import json\n","import pandas as pd\n","import pickle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-Ize90O3OUX"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PKaJam1f3OUX"},"outputs":[],"source":["import torch.nn as nn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DB7kq4RFe6T2"},"outputs":[],"source":["def evaluate(model, dataloader, evaluator, criteria='bleu'):\n","    with torch.no_grad():\n","        quesid2ans = {}\n","        from tqdm.autonotebook import tqdm\n","        with tqdm(range(len(dataloader))) as pbar:\n","          for i, batch in enumerate(dataloader):\n","              ques_ids = batch.pop('question_ids')\n","              batch = {k: v.to(device) for k, v in batch.items()}\n","              results = model.test_step(batch)\n","\n","              pred_ans = results['pred_ans']\n","\n","              for qid, ans in zip(ques_ids, pred_ans):\n","                  quesid2ans[qid] = ans\n","\n","              pbar.update(1)\n","\n","    qid2ans_list = [quesid2ans]\n","    quesid2ans = {}\n","    for qid2ans in qid2ans_list:\n","      for k, v in qid2ans.items():\n","        quesid2ans[k] = v\n","    return evaluator.evaluate_raw(quesid2ans, criteria=criteria)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YOFn9iF3OUX"},"outputs":[],"source":["def pad_zeros(matrix, desired_shape):\n","  if matrix.shape[0] == 0:\n","      return torch.zeros((desired_shape))\n","  # Calculate the amount of padding needed for each dimension\n","  rows_padding = desired_shape[0] - matrix.shape[0]\n","  cols_padding = desired_shape[1] - matrix.shape[1]\n","\n","  # Pad the original tensor with zeros using torch\n","  return torch.nn.functional.pad(matrix, (0, cols_padding, 0, rows_padding), value=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ocfwx7iS3OUX"},"outputs":[],"source":["def csv_to_textdf(src):\n","  df = pd.read_csv(src)\n","  # Initialize an empty list to store the concatenated rows\n","  concatenated_rows = []\n","  headers = list(df.columns)\n","\n","  concatenated_rows.append(' | '.join(headers))\n","  # Iterate through each row of the DataFrame\n","  for index, row in df.iterrows():\n","      # Concatenate the cell values in the row with the '|' separator\n","      concatenated_row = ' | '.join(str(cell) for cell in row)\n","      # Append the concatenated row to the list\n","      concatenated_rows.append(concatenated_row)\n","\n","  # Join all the concatenated rows into a single long string\n","  return ' & '.join(concatenated_rows)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYn7b4wa3OUX"},"outputs":[],"source":["import re\n","import numpy as np\n","import torch\n","import torch.distributed as dist\n","import collections\n","import logging\n","\n","def get_area(pos):\n","    \"\"\"\n","    Args\n","        pos: [B, N, 4]\n","            (x1, x2, y1, y2)\n","\n","    Return\n","        area : [B, N]\n","    \"\"\"\n","    # [B, N]\n","    height = pos[:, :, 3] - pos[:, :, 2]\n","    width = pos[:, :, 1] - pos[:, :, 0]\n","    area = height * width\n","    return area\n","\n","def get_relative_distance(pos):\n","    \"\"\"\n","    Args\n","        pos: [B, N, 4]\n","            (x1, x2, y1, y2)\n","\n","    Return\n","        out : [B, N, N, 4]\n","    \"\"\"\n","    # B, N = pos.size()[:-1]\n","\n","    # [B, N, N, 4]\n","    relative_distance = pos.unsqueeze(1) - pos.unsqueeze(2)\n","\n","    return relative_distance\n","\n","\n","class LossMeter(object):\n","    def __init__(self, maxlen=100):\n","        \"\"\"Computes and stores the running average\"\"\"\n","        self.vals = collections.deque([], maxlen=maxlen)\n","\n","    def __len__(self):\n","        return len(self.vals)\n","\n","    def update(self, new_val):\n","        self.vals.append(new_val)\n","\n","    @property\n","    def val(self):\n","        return sum(self.vals) / len(self.vals)\n","\n","    def __repr__(self):\n","        return str(self.val)\n","\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","def set_global_logging_level(level=logging.ERROR, prefices=[\"\"]):\n","    \"\"\"\n","    Override logging levels of different modules based on their name as a prefix.\n","    It needs to be invoked after the modules have been loaded so that their loggers have been initialized.\n","\n","    Args:\n","        - level: desired level. e.g. logging.INFO. Optional. Default is logging.ERROR\n","        - prefices: list of one or more str prefices to match (e.g. [\"transformers\", \"torch\"]). Optional.\n","          Default is `[\"\"]` to match all active loggers.\n","          The match is a case-sensitive `module_name.startswith(prefix)`\n","    \"\"\"\n","    prefix_re = re.compile(fr'^(?:{ \"|\".join(prefices) })')\n","    for name in logging.root.manager.loggerDict:\n","        if re.match(prefix_re, name):\n","            logging.getLogger(name).setLevel(level)\n","\n","\n","def get_iou(anchors, gt_boxes):\n","    \"\"\"\n","    anchors: (N, 4) torch floattensor\n","    gt_boxes: (K, 4) torch floattensor\n","    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n","    \"\"\"\n","    N = anchors.size(0)\n","\n","    if gt_boxes.size() == (4,):\n","        gt_boxes = gt_boxes.view(1, 4)\n","    K = gt_boxes.size(0)\n","\n","    gt_boxes_area = (\n","        (gt_boxes[:, 2] - gt_boxes[:, 0] + 1) *\n","        (gt_boxes[:, 3] - gt_boxes[:, 1] + 1)\n","    ).view(1, K)\n","\n","    anchors_area = (\n","        (anchors[:, 2] - anchors[:, 0] + 1) *\n","        (anchors[:, 3] - anchors[:, 1] + 1)\n","    ).view(N, 1)\n","\n","    boxes = anchors.view(N, 1, 4).expand(N, K, 4)\n","    query_boxes = gt_boxes.view(1, K, 4).expand(N, K, 4)\n","\n","    iw = (\n","        torch.min(boxes[:, :, 2], query_boxes[:, :, 2])\n","        - torch.max(boxes[:, :, 0], query_boxes[:, :, 0])\n","        + 1\n","    )\n","    iw[iw < 0] = 0\n","\n","    ih = (\n","        torch.min(boxes[:, :, 3], query_boxes[:, :, 3])\n","        - torch.max(boxes[:, :, 1], query_boxes[:, :, 1])\n","        + 1\n","    )\n","    ih[ih < 0] = 0\n","\n","    ua = anchors_area + gt_boxes_area - (iw * ih)\n","    overlaps = iw * ih / ua\n","\n","    return overlaps\n","\n","\n","def xywh_to_xyxy(boxes):\n","    \"\"\"Convert [x y w h] box format to [x1 y1 x2 y2] format.\"\"\"\n","    return np.hstack((boxes[:, 0:2], boxes[:, 0:2] + boxes[:, 2:4] - 1))"]},{"cell_type":"markdown","metadata":{"id":"rD5vFVPo3OUX"},"source":["## Graph Layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q_X4Dpl_3OUX"},"outputs":[],"source":["import math\n","\n","import torch\n","\n","from torch.nn.parameter import Parameter\n","from torch.nn.modules.module import Module\n","\n","\n","class GraphConvolution(Module):\n","    \"\"\"\n","    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n","    \"\"\"\n","\n","    def __init__(self, in_features, out_features, bias=True):\n","        super(GraphConvolution, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n","        if bias:\n","            self.bias = Parameter(torch.FloatTensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        stdv = 1. / math.sqrt(self.weight.size(1))\n","        self.weight.data.uniform_(-stdv, stdv)\n","        if self.bias is not None:\n","            self.bias.data.uniform_(-stdv, stdv)\n","\n","    def forward(self, input, adj, dis):\n","        support = torch.matmul(input, self.weight)\n","        if dis is None:\n","          output = torch.matmul(adj, support)\n","        else:\n","          output = torch.matmul(adj*dis, support)\n","        if self.bias is not None:\n","            return output + self.bias\n","        else:\n","            return output\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + ' (' \\\n","               + str(self.in_features) + ' -> ' \\\n","               + str(self.out_features) + ')'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ltwpEyUC3OUX"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class GCN(nn.Module):\n","    def __init__(self, nfeat, nhid, ofeat, dropout):\n","        super(GCN, self).__init__()\n","\n","        self.ofeat = ofeat\n","        self.dropout = dropout\n","        self.gc1 = GraphConvolution(nfeat, nhid)\n","        self.gc2 = GraphConvolution(nhid, ofeat)\n","\n","    def forward(self, x, adj, dis):\n","        x = F.relu(self.gc1(x, adj, dis))\n","        x = F.dropout(x, self.dropout, training=self.training)\n","        x = self.gc2(x, adj, dis)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mv9m2cDa3OUX"},"outputs":[],"source":["class GraphFuse(nn.Module):\n","    def __init__(self):\n","        super(GraphFuse, self).__init__()\n","        self.fg_gcn = GCN(768,768,768, 0.2)\n","        self.sem_gcn = GCN(768,768,768, 0.2)\n","        self.fc1 = nn.Linear(768*2, 768)\n","        self.fc2 = nn.Linear(768, 768)\n","\n","    def forward(self, ful_adj, sem_adj, ful_weights, sem_weights, vis_feat, text_feat, batch_size, graph_mask):\n","        full_feature = self.fg_gcn(vis_feat, ful_adj, ful_weights).clone()\n","        sem_feature = self.sem_gcn(text_feat, sem_adj, None).clone()\n","\n","        sem_feature = sem_feature * graph_mask\n","        out = torch.cat((full_feature, sem_feature[:, :full_feature.shape[1]]), dim=2)\n","        out = F.relu(self.fc1(out))\n","        out = F.dropout(out, 0.1, training=self.training)\n","        out = F.relu(self.fc2(out))\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"qmA0t0F43OUX"},"source":["## VL-T5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XIb_aO9F3OUX"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vluVkgrD3OUY"},"outputs":[],"source":["from transformers import T5Tokenizer, T5TokenizerFast, PreTrainedTokenizer, PreTrainedTokenizerFast, PreTrainedTokenizerBase\n","import re\n","import sentencepiece as spm\n","\n","class VLT5Tokenizer(T5Tokenizer):\n","\n","    # vocab_files_names = VOCAB_FILES_NAMES\n","    # pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n","    # max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n","    # model_input_names = [\"attention_mask\"]\n","\n","    def __init__(\n","        self,\n","        vocab_file,\n","        eos_token=\"</s>\",\n","        unk_token=\"<unk>\",\n","        pad_token=\"<pad>\",\n","        extra_ids=100,\n","        vis_extra_ids=100,\n","        additional_special_tokens=None,\n","        **kwargs\n","    ):\n","        # Add extra_ids to the special token list\n","        if extra_ids > 0 and additional_special_tokens is None:\n","            additional_special_tokens = [\"<extra_id_{}>\".format(i) for i in range(extra_ids)]\n","        elif extra_ids > 0 and additional_special_tokens is not None:\n","            # Check that we have the right number of extra_id special tokens\n","            extra_tokens = len(set(filter(lambda x: bool(\"extra_id\" in x), additional_special_tokens)))\n","            if extra_tokens != extra_ids:\n","                raise ValueError(\n","                    f\"Both extra_ids ({extra_ids}) and additional_special_tokens ({additional_special_tokens}) are provided to T5Tokenizer. \"\n","                    \"In this case the additional_special_tokens must include the extra_ids tokens\"\n","                )\n","        self.sp_model = spm.SentencePieceProcessor()\n","        self.sp_model.Load(vocab_file)\n","        if vis_extra_ids > 0:\n","            additional_special_tokens.extend([\"<vis_extra_id_{}>\".format(i) for i in range(vis_extra_ids)])\n","\n","        self.vocab_file = vocab_file\n","        self._extra_ids = extra_ids\n","        self._vis_extra_ids = vis_extra_ids\n","\n","        PreTrainedTokenizer.__init__(\n","            self,\n","            eos_token=eos_token,\n","            unk_token=unk_token,\n","            pad_token=pad_token,\n","            extra_ids=extra_ids,\n","            additional_special_tokens=additional_special_tokens,\n","            **kwargs,\n","        )\n","\n","    @property\n","    def vocab_size(self):\n","        return self.sp_model.get_piece_size() + self._extra_ids + self._vis_extra_ids\n","\n","    def get_vocab(self):\n","        vocab = {self.convert_ids_to_tokens(\n","            i): i for i in range(self.vocab_size)}\n","        vocab.update(self.added_tokens_encoder)\n","        return vocab\n","\n","    def _convert_token_to_id(self, token):\n","        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n","        if token.startswith(\"<extra_id_\"):\n","            match = re.match(r\"<extra_id_(\\d+)>\", token)\n","            num = int(match.group(1))\n","            return self.vocab_size - num - 1 - self._vis_extra_ids\n","        elif token.startswith(\"<vis_extra_id_\"):\n","            match = re.match(r\"<vis_extra_id_(\\d+)>\", token)\n","            num = int(match.group(1))\n","            return self.vocab_size - num - 1\n","        return self.sp_model.piece_to_id(token)\n","\n","    def _convert_id_to_token(self, index):\n","        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n","        if index < self.sp_model.get_piece_size():\n","            token = self.sp_model.IdToPiece(index)\n","        else:\n","            if index > self.sp_model.get_piece_size() + self._extra_ids - 1:\n","                token = \"<vis_extra_id_{}>\".format(self.vocab_size - 1 - index)\n","            else:\n","                token = \"<extra_id_{}>\".format(self.vocab_size - self._vis_extra_ids - 1 - index)\n","        return token\n","\n","\n","# Below are for Rust-based Fast Tokenizer\n","\n","from transformers.convert_slow_tokenizer import SpmConverter\n","from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n","from typing import Any, Dict, List, Optional, Tuple, Union\n","\n","\n","class VLT5Converter(SpmConverter):\n","    def vocab(self, proto):\n","        vocab = [(piece.piece, piece.score) for piece in proto.pieces]\n","        num_extra_ids = self.original_tokenizer._extra_ids\n","        vocab += [(\"<extra_id_{}>\".format(i), 0.0)\n","                  for i in range(num_extra_ids - 1, -1, -1)]\n","\n","        num_vis_extra_ids = self.original_tokenizer._vis_extra_ids\n","        vocab += [(\"<vis_extra_id_{}>\".format(i), 0.0)\n","                  for i in range(num_vis_extra_ids - 1, -1, -1)]\n","\n","        return vocab\n","\n","    def post_processor(self):\n","        return processors.TemplateProcessing(\n","            single=[\"$A\", \"</s>\"],\n","            pair=[\"$A\", \"</s>\", \"$B\", \"</s>\"],\n","            special_tokens=[\n","                (\"</s>\", self.original_tokenizer.convert_tokens_to_ids(\"</s>\")),\n","            ],\n","        )\n","\n","\n","def convert_slow_vlt5tokenizer(vlt5tokenizer):\n","    return VLT5Converter(vlt5tokenizer).converted()\n","\n","\n","class VLT5TokenizerFast(T5TokenizerFast):\n","\n","    # vocab_files_names = VOCAB_FILES_NAMES\n","    # pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n","    # max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n","    # model_input_names = [\"attention_mask\"]\n","    slow_tokenizer_class = VLT5Tokenizer\n","\n","    prefix_tokens: List[int] = []\n","\n","    def __init__(\n","        self,\n","        vocab_file,\n","        tokenizer_file=None,\n","        eos_token=\"</s>\",\n","        unk_token=\"<unk>\",\n","        pad_token=\"<pad>\",\n","        extra_ids=100,\n","        vis_extra_ids=100,\n","        additional_special_tokens=None,\n","        **kwargs\n","    ):\n","        # Add extra_ids to the special token list\n","        if extra_ids > 0 and additional_special_tokens is None:\n","            additional_special_tokens = [\"<extra_id_{}>\".format(i) for i in range(extra_ids)]\n","        elif extra_ids > 0 and additional_special_tokens is not None:\n","            # Check that we have the right number of extra_id special tokens\n","            extra_tokens = len(set(filter(lambda x: bool(\"extra_id\" in x), additional_special_tokens)))\n","            if extra_tokens != extra_ids:\n","                raise ValueError(\n","                    f\"Both extra_ids ({extra_ids}) and additional_special_tokens ({additional_special_tokens}) are provided to T5Tokenizer. \"\n","                    \"In this case the additional_special_tokens must include the extra_ids tokens\"\n","                )\n","\n","        if vis_extra_ids > 0:\n","            additional_special_tokens.extend([\"<vis_extra_id_{}>\".format(i) for i in range(vis_extra_ids)])\n","\n","        slow_tokenizer = self.slow_tokenizer_class(\n","            vocab_file,\n","            tokenizer_file=tokenizer_file,\n","            eos_token=eos_token,\n","            unk_token=unk_token,\n","            pad_token=pad_token,\n","            extra_ids=extra_ids,\n","            vis_extra_ids=vis_extra_ids,\n","            # additional_special_tokens=additional_special_tokens,\n","            **kwargs\n","        )\n","        fast_tokenizer = convert_slow_vlt5tokenizer(slow_tokenizer)\n","        self._tokenizer = fast_tokenizer\n","\n","        PreTrainedTokenizerBase.__init__(\n","            self,\n","            tokenizer_file=tokenizer_file,\n","            eos_token=eos_token,\n","            unk_token=unk_token,\n","            pad_token=pad_token,\n","            extra_ids=extra_ids,\n","            vis_extra_ids=vis_extra_ids,\n","            additional_special_tokens=additional_special_tokens,\n","            **kwargs,\n","        )\n","\n","        self.vocab_file = vocab_file\n","        self._extra_ids = extra_ids\n","        self._vis_extra_ids = vis_extra_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F0tszoPM3OUY"},"outputs":[],"source":["from dataclasses import dataclass\n","\n","from transformers.models.t5.modeling_t5 import (\n","    T5Stack, T5Block, T5LayerNorm, T5LayerSelfAttention, T5LayerFF, T5LayerCrossAttention,\n","    T5PreTrainedModel, T5ForConditionalGeneration\n",")\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import CrossEntropyLoss\n","\n","from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple\n","import copy\n","\n","from transformers.modeling_outputs import ModelOutput, BaseModelOutput, BaseModelOutputWithPast, BaseModelOutputWithPastAndCrossAttentions, Seq2SeqLMOutput, Seq2SeqModelOutput\n","from transformers.modeling_utils import PreTrainedModel, find_pruneable_heads_and_indices, prune_linear_layer\n","from transformers.utils import logging\n","from transformers import BeamScorer, BeamSearchScorer\n","\n","# from utils import *\n","\n","logger = logging.get_logger(__name__)\n","\n","\n","class VisualEmbedding(nn.Module):\n","    def __init__(self, config, obj_order_embedding):\n","        super().__init__()\n","        self.config = config\n","        feat_dim = config.feat_dim\n","        pos_dim = config.pos_dim\n","        # n_objs = config.n_objs\n","        n_images = config.n_images\n","\n","        if self.config.individual_vis_layer_norm:\n","\n","            # Object feature encoding\n","            feat_embedding = [nn.Linear(feat_dim, config.d_model)]\n","            if self.config.use_vis_layer_norm:\n","                feat_embedding.append(T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon))\n","            self.feat_embedding = nn.Sequential(*feat_embedding)\n","\n","            # self.relative_vis_pos_embedding = nn.Linear(pos_dim + 1, config.num_heads)\n","            absolute_vis_pos_embedding = [nn.Linear(pos_dim + 1, config.d_model)]\n","            if self.config.use_vis_layer_norm:\n","                absolute_vis_pos_embedding.append(T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon))\n","            self.absolute_vis_pos_embedding = nn.Sequential(*absolute_vis_pos_embedding)\n","            # self.absolute_vis_pos_layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n","\n","            if self.config.use_vis_order_embedding:\n","                # self.obj_order_embedding = nn.Embedding(n_objs, config.d_model)\n","                self.obj_order_embedding = obj_order_embedding\n","                self.img_order_embedding = nn.Embedding(n_images, config.d_model)\n","\n","        else:\n","            # Object feature encoding\n","            feat_embedding = [nn.Linear(feat_dim, config.d_model)]\n","            # if self.config.use_vis_layer_norm:\n","            #     feat_embedding.append(T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon))\n","            self.feat_embedding = nn.Sequential(*feat_embedding)\n","\n","            # self.relative_vis_pos_embedding = nn.Linear(pos_dim + 1, config.num_heads)\n","            absolute_vis_pos_embedding = [nn.Linear(pos_dim + 1, config.d_model)]\n","            # if self.config.use_vis_layer_norm:\n","            #     absolute_vis_pos_embedding.append(T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon))\n","            self.absolute_vis_pos_embedding = nn.Sequential(*absolute_vis_pos_embedding)\n","            # self.absolute_vis_pos_layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n","\n","            if self.config.use_vis_order_embedding:\n","                # self.obj_order_embedding = nn.Embedding(n_objs, config.d_model)\n","                self.obj_order_embedding = obj_order_embedding\n","                self.img_order_embedding = nn.Embedding(n_images, config.d_model)\n","\n","            if self.config.use_vis_layer_norm:\n","                self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n","\n","    def get_area(self, pos):\n","        \"\"\"\n","        Args\n","            pos: [B, N, 4]\n","                (x1, x2, y1, y2)\n","        Return\n","            area : [B, N]\n","        \"\"\"\n","        # [B, N]\n","        height = pos[:, :, 3] - pos[:, :, 2]\n","        width = pos[:, :, 1] - pos[:, :, 0]\n","        area = height * width\n","        return area\n","\n","\n","    def forward(self, feats, pos, img_order_ids=None, obj_order_ids=None):\n","        \"\"\"\n","        Args\n","            feats: [B, N, feat_dim]\n","            pos: [B, N, 4]\n","                (x1, x2, y1, y2)\n","        Return\n","            relative_vis_pos_embedding: [B, N, N, n_heads]\n","            absolute_vis_pos_embedding: # [B, N, d_model]\n","        \"\"\"\n","\n","        B, N, _ = feats.size()\n","        assert pos.size() == (B, N, 4)\n","\n","        feat_embedding = self.feat_embedding(feats)\n","\n","        device = feats.device\n","        dtype = feats.dtype\n","\n","        area = self.get_area(pos).unsqueeze(2) # [B, N, 1]\n","        pos = torch.cat([pos, area], dim=2) # [B, N, 5]\n","\n","        # [B, N, d_model]\n","        absolute_vis_pos_embedding = self.absolute_vis_pos_embedding(pos)\n","        # absolute_vis_pos_embedding = self.absolute_vis_pos_layer_norm(absolute_vis_pos_embedding)\n","\n","\n","        if self.config.use_vis_order_embedding:\n","            if img_order_ids is None:\n","                img_order_ids = torch.zeros(N, dtype=torch.long, device=device)\n","                img_order_ids = img_order_ids.unsqueeze(0) #.expand(B, -1)\n","            img_order_embedding = self.img_order_embedding(img_order_ids)\n","\n","            if obj_order_ids is None:\n","                obj_order_ids = torch.arange(N, dtype=torch.long, device=device)\n","                obj_order_ids = obj_order_ids.unsqueeze(0) #.expand(B,-1)\n","            # assert obj_order_ids.max().item() < 32200, obj_order_ids\n","            obj_order_ids = self.obj_order_embedding.num_embeddings - obj_order_ids - 1\n","            obj_order_embedding = self.obj_order_embedding(obj_order_ids)\n","\n","            vis_embedding = feat_embedding + absolute_vis_pos_embedding + \\\n","                img_order_embedding + obj_order_embedding\n","\n","        else:\n","            vis_embedding = feat_embedding + absolute_vis_pos_embedding\n","\n","        if not self.config.individual_vis_layer_norm:\n","            if self.config.use_vis_layer_norm:\n","                vis_embedding = self.layer_norm(vis_embedding)\n","\n","        return vis_embedding\n","\n","\n","class JointEncoder(T5Stack):\n","    def __init__(self, config, embed_tokens=None):\n","        super(T5Stack, self).__init__(config)\n","        self.config = config\n","\n","        self.embed_tokens = embed_tokens\n","        self.is_decoder = self.config.is_decoder\n","        assert self.config.is_decoder is False\n","\n","        self.visual_embedding = VisualEmbedding(self.config, embed_tokens)\n","\n","        self.block = nn.ModuleList(\n","            [T5Block(config, has_relative_attention_bias=(i == 0))\n","                for i in range(config.num_layers)]\n","        )\n","        self.final_layer_norm = T5LayerNorm(\n","            config.d_model, eps=config.layer_norm_epsilon)\n","        self.dropout = nn.Dropout(config.dropout_rate)\n","\n","        self.init_weights()\n","        self.model_parallel = False\n","        self.device_map = None\n","\n","    def set_input_embeddings(self, new_embeddings):\n","        self.embed_tokens = new_embeddings\n","        self.visual_embedding.obj_order_embedding = new_embeddings\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","\n","        vis_inputs=None,\n","        vis_attention_mask=None,\n","\n","        inputs_embeds=None,\n","        head_mask=None,\n","        past_key_values=None,\n","        use_cache=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","\n","        if inputs_embeds is None:\n","            assert self.embed_tokens is not None, \"You have to initialize the model with valid token embeddings\"\n","            inputs_embeds = self.embed_tokens(input_ids)\n","\n","        B, L = inputs_embeds.size()[:-1]\n","\n","        vis_feats = vis_inputs[0]\n","        boxes = vis_inputs[1]\n","        img_order_ids = None\n","        obj_order_ids = None\n","        if len(vis_inputs) >= 3:\n","            img_order_ids = vis_inputs[2]\n","        if len(vis_inputs) == 4:\n","            obj_order_ids = vis_inputs[3]\n","\n","        vis_embeds = self.visual_embedding(\n","            vis_feats, boxes, img_order_ids, obj_order_ids)\n","\n","        V_L = vis_embeds.size(1)\n","        inputs_embeds = torch.cat([inputs_embeds, vis_embeds], dim=1)\n","        if attention_mask is None:\n","            attention_mask = input_ids.ne(self.config.pad_token_id).to(dtype=inputs_embeds.dtype, device=inputs_embeds.device)\n","\n","        if vis_attention_mask is None:\n","            vis_attention_mask = attention_mask.new_ones(B, V_L)\n","\n","        attention_mask = torch.cat([attention_mask, vis_attention_mask], dim=1)\n","\n","        # ourselves in which case we just need to make it broadcastable to all heads.\n","        extended_attention_mask = self.get_extended_attention_mask(\n","            attention_mask,\n","            (B, L+V_L),\n","            inputs_embeds.device)\n","\n","        # initialize past_key_values with `None` if past does not exist\n","        if past_key_values is None:\n","            past_key_values = [None] * len(self.block)\n","\n","        # Prepare head mask if needed\n","        head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n","        present_key_value_states = () if use_cache else None\n","        all_hidden_states = () if output_hidden_states else None\n","        all_attentions = () if output_attentions else None\n","        all_cross_attentions = () if (output_attentions and self.is_decoder) else None\n","        # position_bias = None\n","        # encoder_decoder_position_bias = None\n","\n","        hidden_states = self.dropout(inputs_embeds)\n","\n","        if self.config.num_layers > 0:\n","\n","            assert self.block[0].layer[0].SelfAttention.has_relative_attention_bias\n","\n","            seq_length = L + V_L\n","            q_len = seq_length\n","            k_len = seq_length\n","\n","            # [1, n_heads, Q_len, K_len]\n","            text_position_bias = self.block[0].layer[0].SelfAttention.compute_bias(\n","                L, L)\n","            num_heads = text_position_bias.size(1)\n","            position_bias = text_position_bias.new_zeros(\n","                1, num_heads, seq_length, seq_length)\n","            position_bias[:, :, :L, :L] = text_position_bias\n","\n","            # print('position_bias size', position_bias.size())\n","            # print('attention_mask size', attention_mask.size())\n","            # print('extended_attention_mask size', extended_attention_mask.size())\n","            # relative position bias only between Text <-> Text\n","            # no relative position bias Text -> Vision\n","            # no relative position bias Vision -> Text\n","            # no relative position bias Vision <-> Vision\n","            # position_bias[:, :, L:, :] = 0\n","            # position_bias[:, :, :, L:] = 0\n","            position_bias = position_bias + extended_attention_mask\n","\n","            for i, (layer_module, past_key_value) in enumerate(zip(self.block, past_key_values)):\n","\n","                # if output_hidden_states:\n","                #     all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","                layer_outputs = layer_module(\n","                    hidden_states,\n","                    attention_mask=extended_attention_mask,\n","                    position_bias=position_bias,\n","                    encoder_hidden_states=None,\n","                    encoder_attention_mask=None,\n","                    encoder_decoder_position_bias=None,\n","                    layer_head_mask=head_mask[i],\n","                    past_key_value=past_key_value,\n","                    use_cache=use_cache,\n","                    output_attentions=output_attentions,\n","                )\n","                if use_cache is None:\n","                  layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n","                # layer_outputs is a tuple with:\n","                # hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n","                hidden_states, present_key_value_state = layer_outputs[:2]\n","\n","                # We share the position biases between the layers - the first layer store them\n","                # layer_outputs = hidden-states, key-value-states (self-attention weights),\n","                # (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n","                position_bias = layer_outputs[2]\n","\n","                # append next layer key value states\n","                if use_cache:\n","                    present_key_value_states = present_key_value_states + \\\n","                        (present_key_value_state,)\n","\n","                # if output_attentions:\n","                #     all_attentions = all_attentions + (layer_outputs[3],)\n","                #     if self.is_decoder:\n","                #         all_cross_attentions = all_cross_attentions + \\\n","                #             (layer_outputs[5],)\n","\n","        hidden_states = self.final_layer_norm(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","\n","        # Add last layer\n","        if output_hidden_states:\n","            all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","        if not return_dict:\n","            return tuple(\n","                v\n","                for v in [\n","                    hidden_states,\n","                    present_key_value_states,\n","                    all_hidden_states,\n","                    all_attentions,\n","                    all_cross_attentions,\n","                ]\n","                if v is not None\n","            )\n","        return BaseModelOutputWithPastAndCrossAttentions(\n","            last_hidden_state=hidden_states,\n","            past_key_values=present_key_value_states,\n","            hidden_states=all_hidden_states,\n","            attentions=all_attentions,\n","            cross_attentions=all_cross_attentions,\n","        )\n","\n","\n","class VLT5(T5ForConditionalGeneration):\n","    _keys_to_ignore_on_load_missing = [\n","        r\"encoder\\.embed_tokens\\.weight\",\n","        r\"decoder\\.embed_tokens\\.weight\",\n","        r\"lm_head\\.weight\",\n","    ]\n","    _keys_to_ignore_on_load_unexpected = [\n","        r\"decoder\\.block\\.0\\.layer\\.1\\.EncDecAttention\\.relative_attention_bias\\.weight\",\n","    ]\n","\n","    def __init__(self, config):\n","        super(T5ForConditionalGeneration, self).__init__(config)\n","\n","        self.config = config\n","\n","        self.model_dim = config.d_model\n","\n","        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n","\n","        encoder_config = copy.deepcopy(config)\n","        encoder_config.is_decoder = False\n","        encoder_config.use_cache = False\n","        encoder_config.is_encoder_decoder = False\n","\n","        #---- Modified ----#\n","        # self.encoder = T5Stack(encoder_config, self.shared)\n","        self.encoder = JointEncoder(encoder_config, self.shared)\n","        #------------------#\n","\n","        decoder_config = copy.deepcopy(config)\n","        decoder_config.is_decoder = True\n","        decoder_config.is_encoder_decoder = False\n","\n","        self.decoder = T5Stack(decoder_config, self.shared)\n","\n","        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n","\n","        self.init_weights()\n","        self.gf = GraphFuse()\n","        self.proj = nn.Linear(768, 768)\n","        self.norm = nn.LayerNorm(768)\n","        # Model parallel\n","        self.model_parallel = False\n","        self.device_map = None\n","\n","    def set_input_embeddings(self, new_embeddings):\n","        self.shared = new_embeddings\n","        self.encoder.set_input_embeddings(new_embeddings)\n","        self.decoder.set_input_embeddings(new_embeddings)\n","\n","    def extend_vocab(self, vocab_size):\n","\n","        new_shared = nn.Embedding(vocab_size, self.config.d_model)\n","        old_weight = self.shared.weight.data.detach().clone()\n","        old_vocab_size = old_weight.size(0)\n","        new_shared.weight.data[:old_vocab_size, :] = old_weight\n","        self.shared = new_shared\n","\n","        new_lm_head = nn.Linear(self.config.d_model, vocab_size, bias=False)\n","        old_weight = self.lm_head.weight.data.detach().clone()\n","        old_vocab_size = old_weight.size(0)\n","        new_lm_head.weight.data[:old_vocab_size, :] = old_weight\n","        self.lm_head = new_lm_head\n","\n","        self.vis_encoder.visual_embedding.obj_order_embedding = self.shared\n","\n","        self.encoder.embed_tokens = self.shared\n","        self.decoder.embed_tokens = self.shared\n","\n","        self.lm_head.weight = self.shared.weight\n","\n","        self.config.vocab_size = vocab_size\n","        self.encoder.config.vocab_size = vocab_size\n","        self.vis_encoder.config.vocab_size = vocab_size\n","        self.decoder.config.vocab_size = vocab_size\n","\n","\n","    # @add_start_docstrings_to_callable(T5_INPUTS_DOCSTRING)\n","    # @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        encoder_outputs=None,\n","\n","        vis_inputs=None,\n","        vis_attention_mask=None,\n","\n","        decoder_input_ids=None,\n","        decoder_attention_mask=None,\n","        past_key_values=None,\n","        use_cache=None,\n","        labels=None,\n","        inputs_embeds=None,\n","        decoder_inputs_embeds=None,\n","        head_mask=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","        reduce_loss=False,\n","        return_hidden_state=False,\n","        full_adj = None,\n","        sem_adj = None,\n","        full_weights = None,\n","        rtexts_feats = None,\n","        graph_mask = None,\n","        **kwargs,\n","    ):\n","\n","        use_cache = use_cache if use_cache is not None else self.config.use_cache\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        if encoder_outputs is None:\n","\n","            encoder_outputs = self.encoder(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","                inputs_embeds=inputs_embeds,\n","\n","                vis_inputs=vis_inputs,\n","                vis_attention_mask=vis_attention_mask,\n","\n","                head_mask=head_mask,\n","                output_attentions=output_attentions,\n","                output_hidden_states=output_hidden_states,\n","                return_dict=return_dict,\n","            )\n","\n","        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n","            encoder_outputs = BaseModelOutput(\n","                last_hidden_state=encoder_outputs[0],\n","                hidden_states=encoder_outputs[1] if len(\n","                    encoder_outputs) > 1 else None,\n","                attentions=encoder_outputs[2] if len(\n","                    encoder_outputs) > 2 else None,\n","            )\n","\n","        hidden_states = encoder_outputs[0]\n","        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n","            # get decoder inputs from shifting lm labels to the right\n","            decoder_input_ids = self._shift_right(labels)\n","\n","        # If decoding with past key value states, only the last tokens\n","        # should be given as an input\n","        if past_key_values is not None:\n","            assert labels is None, \"Decoder should not use cached key value states when training.\"\n","            if decoder_input_ids is not None:\n","                decoder_input_ids = decoder_input_ids[:, -1:]\n","            if decoder_inputs_embeds is not None:\n","                decoder_inputs_embeds = decoder_inputs_embeds[:, -1:]\n","\n","        if attention_mask is None:\n","            attention_mask = input_ids.ne(self.config.pad_token_id).to(dtype=hidden_states.dtype, device=hidden_states.device)\n","        if vis_attention_mask is None:\n","            B, L = attention_mask.size()\n","            V_L = encoder_outputs[0].size(1) - L\n","            vis_attention_mask = attention_mask.new_ones(B, V_L)\n","        encoder_attention_mask = torch.cat([attention_mask, vis_attention_mask], dim=1)\n","\n","        obj_features = hidden_states[:, -36:]\n","        gf_feats = self.gf(full_adj, sem_adj, full_weights, None, obj_features, self.proj(rtexts_feats), full_adj.shape[0],graph_mask)\n","        gf_feats_input = torch.zeros_like(hidden_states)\n","        gf_feats_input[:, -36:] = gf_feats\n","        hidden_states += gf_feats_input\n","\n","        # Decode\n","        decoder_outputs = self.decoder(\n","            input_ids=decoder_input_ids,\n","            attention_mask=decoder_attention_mask,\n","            inputs_embeds=decoder_inputs_embeds,\n","            past_key_values=past_key_values,\n","\n","            encoder_hidden_states=hidden_states,\n","            encoder_attention_mask=encoder_attention_mask,\n","\n","            head_mask=head_mask,\n","            use_cache=use_cache,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        # print('decoder_outputs')\n","        # print(decoder_outputs)\n","\n","        sequence_output = decoder_outputs[0]\n","\n","        assert self.config.tie_word_embeddings is True\n","\n","        if self.config.tie_word_embeddings:\n","            # Rescale output before projecting on vocab\n","            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n","            sequence_output = sequence_output * (self.model_dim ** -0.5)\n","\n","        if return_hidden_state:\n","            return sequence_output\n","\n","        lm_logits = self.lm_head(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            # loss_fct = CrossEntropyLoss(ignore_index=-100)\n","            # loss = loss_fct(\n","            #     lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n","            # TODO(thom): Add z_loss https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L666\n","\n","            if reduce_loss:\n","                loss_fct = CrossEntropyLoss(ignore_index=-100)\n","            else:\n","                loss_fct = CrossEntropyLoss(ignore_index=-100, reduction='none')\n","            loss = loss_fct(\n","                lm_logits.view(-1, lm_logits.size(-1)),\n","                labels.view(-1))\n","\n","            # print('loss')\n","            # print(loss)\n","\n","        # if not return_dict:\n","        #     output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n","        #     return ((loss,) + output) if loss is not None else output\n","\n","        return VLSeq2SeqLMOutput(\n","            loss=loss,\n","            logits=lm_logits,\n","            past_key_values=decoder_outputs.past_key_values,\n","            decoder_last_hidden_state=decoder_outputs.last_hidden_state,\n","            decoder_hidden_states=decoder_outputs.hidden_states,\n","            # decoder_attentions=decoder_outputs.attentions,\n","            # encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n","            # encoder_hidden_states=encoder_outputs.hidden_states,\n","            # encoder_attentions=encoder_outputs.attentions,\n","            # vis_encoder_last_hidden_state=vis_encoder_outputs.last_hidden_state,\n","            # vis_encoder_hidden_states=vis_encoder_outputs.hidden_states,\n","            # vis_encoder_attentions=vis_encoder_outputs.attentions,\n","            # cross_encoder_outputs=cross_encoder_outputs\n","        )\n","\n","    def prepare_inputs_for_generation(\n","        self, input_ids, past=None, attention_mask=None, use_cache=None,\n","        encoder_outputs=None,\n","        full_adj = None,\n","        sem_adj = None,\n","        full_weights = None,\n","        rtexts_feats = None,\n","        graph_mask = None,\n","        **kwargs):\n","\n","        # cut decoder_input_ids if past is used\n","        if past is not None:\n","            input_ids = input_ids[:, -1:]\n","\n","        output = {\n","            \"decoder_input_ids\": input_ids,\n","            \"past_key_values\": past,\n","            \"encoder_outputs\": encoder_outputs,\n","            \"attention_mask\": attention_mask,\n","            \"use_cache\": use_cache,\n","            \"full_adj\": full_adj,\n","            \"sem_adj\": sem_adj,\n","            \"full_weights\": full_weights,\n","            \"rtexts_feats\": rtexts_feats,\n","            \"graph_mask\": graph_mask,\n","        }\n","\n","        if 'vis_attention_mask' in kwargs:\n","            output['vis_attention_mask'] = kwargs['vis_attention_mask']\n","\n","        return output\n","\n","    @staticmethod\n","    def _expand_inputs_for_generation(\n","        input_ids: torch.LongTensor,\n","        expand_size: int = 1,\n","        is_encoder_decoder: bool = False,\n","        attention_mask: torch.LongTensor = None,\n","        encoder_outputs: ModelOutput = None,\n","        **model_kwargs\n","    ) -> Tuple[torch.LongTensor, Dict[str, Any]]:\n","        expanded_return_idx = (\n","            torch.arange(input_ids.shape[0]).view(-1, 1).repeat(1,\n","                                                                expand_size).view(-1).to(input_ids.device)\n","        )\n","        input_ids = input_ids.index_select(0, expanded_return_idx)\n","\n","        if \"token_type_ids\" in model_kwargs:\n","            token_type_ids = model_kwargs[\"token_type_ids\"]\n","            model_kwargs[\"token_type_ids\"] = token_type_ids.index_select(\n","                0, expanded_return_idx)\n","\n","        if attention_mask is not None:\n","            model_kwargs[\"attention_mask\"] = attention_mask.index_select(\n","                0, expanded_return_idx)\n","\n","        if model_kwargs.get(\"vis_attention_mask\", None) is not None:\n","            model_kwargs['vis_attention_mask'] = model_kwargs['vis_attention_mask'].index_select(\n","                0, expanded_return_idx)\n","\n","        if is_encoder_decoder:\n","            assert encoder_outputs is not None\n","            encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n","                0, expanded_return_idx\n","            )\n","            model_kwargs[\"encoder_outputs\"] = encoder_outputs\n","\n","        full_adj = model_kwargs['full_adj']\n","        model_kwargs['full_adj'] = full_adj.index_select(0, expanded_return_idx)\n","\n","        sem_adj = model_kwargs['sem_adj']\n","        model_kwargs['sem_adj'] = sem_adj.index_select(0, expanded_return_idx)\n","\n","        full_weights = model_kwargs['full_weights']\n","        model_kwargs['full_weights'] = full_weights.index_select(0, expanded_return_idx)\n","\n","        rtexts_feats = model_kwargs['rtexts_feats']\n","        model_kwargs['rtexts_feats'] = rtexts_feats.index_select(0, expanded_return_idx)\n","\n","        graph_mask = model_kwargs['graph_mask']\n","        model_kwargs['graph_mask'] = graph_mask.index_select(0, expanded_return_idx)\n","\n","        return input_ids, model_kwargs\n","\n","\n","@dataclass\n","class VLSeq2SeqLMOutput(ModelOutput):\n","    \"\"\"\n","    Base class for sequence-to-sequence language models outputs.\n","\n","    Args:\n","        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`labels` is provided):\n","            Languaged modeling loss.\n","        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, config.vocab_size)`):\n","            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n","        past_key_values (:obj:`List[torch.FloatTensor]`, `optional`, returned when ``use_cache=True`` is passed or when ``config.use_cache=True``):\n","            List of :obj:`torch.FloatTensor` of length :obj:`config.n_layers`,  with each tensor of shape\n","            :obj:`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).\n","\n","            Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n","            used (see ``past_key_values`` input) to speed up sequential decoding.\n","        decoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n","            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n","            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n","\n","            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n","        decoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n","            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n","            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n","\n","            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n","            self-attention heads.\n","        encoder_last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n","            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n","        encoder_hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_hidden_states=True`` is passed or when ``config.output_hidden_states=True``):\n","            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n","            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n","\n","            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n","        encoder_attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``output_attentions=True`` is passed or when ``config.output_attentions=True``):\n","            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n","            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n","\n","            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n","            self-attention heads.\n","    \"\"\"\n","\n","    loss: Optional[torch.FloatTensor] = None\n","    logits: torch.FloatTensor = None\n","    past_key_values: Optional[List[torch.FloatTensor]] = None\n","    decoder_last_hidden_state: Optional[Tuple[torch.FloatTensor]] = None\n","    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n","    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","\n","    vis_encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n","    vis_encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n","    vis_encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n","\n","    # cross_encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None"]},{"cell_type":"markdown","metadata":{"id":"kTJ_SZVW3OUZ"},"source":["### VL-T5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUn7XjlO3OUZ"},"outputs":[],"source":["from pathlib import Path\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","class VLT5VQA(VLT5):\n","    def __init__(self, config, tokenizer, num_answers=None, label2ans=None):\n","        super().__init__(config)\n","\n","        if config.classifier:\n","            self.answer_head = nn.Sequential(\n","                nn.Linear(config.d_model, config.d_model * 2),\n","                nn.GELU(),\n","                nn.LayerNorm(config.d_model * 2),\n","                nn.Linear(config.d_model * 2, num_answers)\n","            )\n","\n","        self.num_answers = num_answers\n","        self.label2ans = label2ans\n","        self.bce_loss = nn.BCEWithLogitsLoss()\n","        self.tokenizer = tokenizer\n","\n","    def train_step(self, batch):\n","        device = next(self.parameters()).device\n","        vis_feats = batch['vis_feats']\n","        input_ids = batch['input_ids']\n","        vis_pos = batch['boxes']\n","        full_adj = batch['full_adj']\n","        sem_adj = batch['sem_adj']\n","        full_weights = batch['full_weights']\n","        rtexts_feats = batch['rtexts_feats']\n","        graph_mask = batch['graph_mask']\n","\n","        if self.config.classifier:\n","            B = len(input_ids)\n","\n","            decoder_input_ids = torch.ones(\n","                B, 1, dtype=torch.long, device=device) * self.config.decoder_start_token_id\n","\n","            output = self(\n","                input_ids=input_ids,\n","                vis_inputs=(vis_feats, vis_pos),\n","                decoder_input_ids=decoder_input_ids,\n","                output_hidden_states=True,\n","                return_dict=True\n","            )\n","            target = batch['targets'].to(device)\n","\n","            last_layer_hidden_state = output.decoder_hidden_states[-1]\n","            last_hidden_state = last_layer_hidden_state.view(B, -1, self.config.d_model)[:, -1]\n","\n","            # [B, num_answers]\n","            logit = self.answer_head(last_hidden_state)\n","\n","            loss = self.bce_loss(logit, target)\n","\n","        else:\n","            lm_labels = batch[\"target_ids\"].to(device)\n","\n","            output = self(\n","                input_ids=input_ids,\n","                vis_inputs=(vis_feats, vis_pos),\n","                labels=lm_labels,\n","                return_dict=True,\n","                full_adj = full_adj,\n","                sem_adj = sem_adj,\n","                full_weights = full_weights,\n","                rtexts_feats = rtexts_feats,\n","                graph_mask = graph_mask,\n","            )\n","            assert 'loss' in output\n","\n","            lm_mask = (lm_labels != -100).float()\n","            B, L = lm_labels.size()\n","\n","            loss = output['loss']\n","\n","            loss = loss.view(B, L) * lm_mask\n","\n","            loss = loss.sum(dim=1) / lm_mask.sum(dim=1).clamp(min=1)  # B\n","\n","            loss = loss * batch['scores'].to(device=device)\n","\n","            loss = loss.mean()\n","\n","        result = {\n","            'loss': loss\n","        }\n","\n","        return result\n","\n","    @torch.no_grad()\n","    def test_step(self, batch, **kwargs):\n","        self.eval()\n","        device = next(self.parameters()).device\n","        vis_feats = batch['vis_feats']\n","        input_ids = batch['input_ids']\n","        vis_pos = batch['boxes']\n","        full_adj = batch['full_adj']\n","        sem_adj = batch['sem_adj']\n","        full_weights = batch['full_weights']\n","        rtexts_feats = batch['rtexts_feats']\n","        graph_mask = batch['graph_mask']\n","        result = {}\n","        if self.config.classifier:\n","            B = len(input_ids)\n","\n","            decoder_input_ids = torch.ones(\n","                B, 1, dtype=torch.long, device=device) * self.config.decoder_start_token_id\n","\n","            output = self(\n","                input_ids=input_ids,\n","                vis_inputs=(vis_feats, vis_pos),\n","                decoder_input_ids=decoder_input_ids,\n","                output_hidden_states=True,\n","                return_dict=True\n","            )\n","\n","            last_layer_hidden_state = output.decoder_hidden_states[-1]\n","            last_hidden_state = last_layer_hidden_state.view(B, -1, self.config.d_model)[:, -1]\n","\n","            # [B, num_answers]\n","            logit = self.answer_head(last_hidden_state)\n","\n","            score, pred_ans_id = logit.max(1)\n","            pred_ans_id = pred_ans_id.cpu().numpy()\n","            pred_ans = [self.label2ans[ans_id] for ans_id in pred_ans_id]\n","\n","            result['pred_ans'] = pred_ans\n","\n","        else:\n","            output = self.generate(\n","                input_ids=input_ids,\n","                vis_inputs=(vis_feats, vis_pos),\n","                num_beams = 4,\n","                max_length = 128,\n","                full_adj = full_adj,\n","                sem_adj = sem_adj,\n","                full_weights = full_weights,\n","                rtexts_feats = rtexts_feats,\n","                graph_mask = graph_mask,\n","                **kwargs\n","            )\n","            generated_sents = self.tokenizer.batch_decode(output, skip_special_tokens=True)\n","            result['token_ids'] = output\n","            result['pred_ans'] = generated_sents\n","\n","        return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qoz70MQV3OUZ"},"outputs":[],"source":["def create_config():\n","    from transformers import T5Config, BartConfig\n","    config_class = T5Config\n","    config = config_class.from_pretrained('t5-base')\n","\n","    config.feat_dim = 2048\n","    config.pos_dim = 4\n","    config.n_images = 2\n","\n","    config.use_vis_order_embedding = True\n","\n","    config.dropout_rate = 0.1\n","    config.dropout = 0.1\n","    config.attention_dropout = 0.1\n","    config.activation_dropout = 0.1\n","\n","    config.use_vis_layer_norm = True\n","    config.individual_vis_layer_norm = True\n","    config.losses = 'lm,obj,attr,feat'\n","\n","    config.share_vis_lang_layer_norm = True\n","    config.classifier = False\n","\n","    return config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ESKcxEYt2b9q"},"outputs":[],"source":["def dump_result(quesid2ans: dict, path):\n","        \"\"\"\n","        Dump results to a json file, which could be submitted to the VQA online evaluation.\n","        VQA json file submission requirement:\n","            results = [result]\n","            result = {\n","                \"question_id\": int,\n","                \"answer\": str\n","            }\n","        :param quesid2ans: dict of quesid --> ans\n","        :param path: The desired path of saved file.\n","        \"\"\"\n","        with open(path, 'w') as f:\n","            result = []\n","            for ques_id, ans in quesid2ans.items():\n","                result.append({\n","                    'question_id': ques_id,\n","                    'answer': ans\n","                })\n","            json.dump(result, f, indent=4, sort_keys=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oleRTTab3OUa"},"outputs":[],"source":["config = create_config()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E5EzC0I-3OUa"},"outputs":[],"source":["from transformers import T5Tokenizer, BartTokenizer, T5TokenizerFast, BartTokenizerFast"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ylIeGbBG3OUa"},"outputs":[],"source":["vlt5_tokenizer = VLT5TokenizerFast.from_pretrained(\n","    't5-base',\n","    do_lower_case=True,\n","    return_tensors = 'pt'\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fz1AEjWy3OUa"},"outputs":[],"source":["model = VLT5VQA.from_pretrained('t5-base', config=config, tokenizer=vlt5_tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NsvfmXFx3OUa"},"outputs":[],"source":["model.resize_token_embeddings(vlt5_tokenizer.vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PSgCFTF53OUa"},"outputs":[],"source":["from pprint import pprint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PM3n-RqF3OUa"},"outputs":[],"source":["def load_checkpoint(model, ckpt_path):\n","    state_dict = load_state_dict(ckpt_path, 'cpu')\n","\n","    original_keys = list(state_dict.keys())\n","    for key in original_keys:\n","        if key.startswith(\"vis_encoder.\"):\n","            new_key = 'encoder.' + key[len(\"vis_encoder.\"):]\n","            state_dict[new_key] = state_dict.pop(key)\n","\n","        if key.startswith(\"model.vis_encoder.\"):\n","            new_key = 'model.encoder.' + key[len(\"model.vis_encoder.\"):]\n","            state_dict[new_key] = state_dict.pop(key)\n","\n","    results = model.load_state_dict(state_dict, strict=False)\n","    print('Model loaded from ', ckpt_path)\n","    pprint(results)"]},{"cell_type":"markdown","metadata":{"id":"GUlc9fTY3OUb"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qxHY3e7s3OUb"},"outputs":[],"source":["import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXhFVxRn3OUb"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","import random\n","class GraphDataset(Dataset):\n","    def __init__(self, df, pad_len, tokenizer):\n","        self.df = df\n","        self.pad_len = pad_len\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        item = self.df.loc[idx]\n","        sem_data = pickle.load(open(item['graph_sem'], 'rb'))\n","        base_data = pickle.load(open(item['graph_base'], 'rb'))\n","        feats = pickle.load(open(item['feature'], 'rb'))\n","        text_data = pickle.load(open(item['text'], 'rb'))\n","\n","        full_weights = torch.tensor(base_data['full_weights'])[:self.pad_len, :self.pad_len]\n","        full_adj = torch.tensor(base_data['normalized_adj'])[:self.pad_len, :self.pad_len]\n","        sem_adj = torch.tensor(sem_data['normalized_adj'])\n","        vis_feats = torch.tensor(feats['visual_feats'], dtype=torch.float32)[:self.pad_len]\n","\n","        bboxes = torch.tensor(feats['bboxes'], dtype=torch.float32)[:self.pad_len]\n","        bboxes = pad_zeros(bboxes, (self.pad_len, 4))\n","\n","        label_feats = torch.tensor(text_data['label_feature'], dtype=torch.float32)\n","        rtexts_feats = torch.tensor(text_data['text_feature'], dtype=torch.float32)\n","        text_feats = torch.cat((label_feats, rtexts_feats), dim=0)\n","\n","        graph_mask = torch.zeros(sem_adj.shape[0], 768)\n","        graph_mask[:full_adj.shape[0]] = 1\n","\n","        input_ids = self.tokenizer(item['input_texts'], truncation=True, max_length=512)['input_ids']\n","        target_ids = self.tokenizer(item['answers'])['input_ids']\n","\n","        # Extract corresponding rows from the two matrices using the selected row indices\n","        full_weights = pad_zeros(full_weights, (self.pad_len,self.pad_len))\n","        full_adj = pad_zeros(full_adj, (self.pad_len,self.pad_len))\n","        vis_feats = pad_zeros(vis_feats, (self.pad_len, 2048))\n","        question_ids = item['question_ids']\n","\n","        input = {'full_weights': full_weights, 'full_adj': full_adj, 'sem_adj': sem_adj, 'scores': [1], 'rtexts_feats': text_feats, 'vis_feats': vis_feats,\n","                 'boxes': bboxes, 'input_ids': input_ids, 'target_ids': target_ids, 'question_ids': question_ids,\n","                 'graph_mask': graph_mask}\n","\n","        return input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hsYWsv4s3OUb"},"outputs":[],"source":["def collator(batch):\n","  input_batch = {'full_weights': None, 'full_adj': None, 'sem_adj': None, 'input_ids': [], 'target_ids': [], 'scores': [], 'graph_mask': None,\n","                 'rtexts_feats': None, 'vis_feats': None, 'boxes': None, 'question_ids': []}\n","\n","  max_input = max([len(b['input_ids']) for b in batch])\n","  max_target = max([len(b['target_ids']) for b in batch])\n","  max_texts = max(max([b['rtexts_feats'].shape[0] for b in batch]), 36)\n","  max_mask = max(max([b['graph_mask'].shape[0] for b in batch]), 36)\n","  max_sem = max(max([b['sem_adj'].shape[0] for b in batch]), 36)\n","\n","  for b in batch:\n","    if input_batch['full_weights'] is None:\n","      input_batch['full_weights'] = b['full_weights'].unsqueeze(0)\n","    else:\n","      input_batch['full_weights'] = torch.cat((input_batch['full_weights'], b['full_weights'].unsqueeze(0)))\n","\n","    if input_batch['full_adj'] is None:\n","      input_batch['full_adj'] = b['full_adj'].unsqueeze(0)\n","    else:\n","      input_batch['full_adj'] = torch.cat((input_batch['full_adj'], b['full_adj'].unsqueeze(0)))\n","\n","    padded_graph_mask = pad_zeros(b['graph_mask'], (max_mask, 768)).unsqueeze(0)\n","    if input_batch['graph_mask'] is None:\n","      input_batch['graph_mask'] = padded_graph_mask\n","    else:\n","      input_batch['graph_mask'] = torch.cat((input_batch['graph_mask'], padded_graph_mask))\n","\n","    padded_sem_adj = pad_zeros(b['sem_adj'], (max_sem, max_sem)).unsqueeze(0)\n","    if input_batch['sem_adj'] is None:\n","      input_batch['sem_adj'] = padded_sem_adj\n","    else:\n","      input_batch['sem_adj'] = torch.cat((input_batch['sem_adj'], padded_sem_adj))\n","\n","    padded_rtexts_feats = pad_zeros(b['rtexts_feats'], (max_texts, 768)).unsqueeze(0)\n","    if input_batch['rtexts_feats'] is None:\n","      input_batch['rtexts_feats'] = padded_rtexts_feats\n","    else:\n","      input_batch['rtexts_feats'] = torch.cat((input_batch['rtexts_feats'], padded_rtexts_feats))\n","\n","    if input_batch['vis_feats'] is None:\n","      input_batch['vis_feats'] = b['vis_feats'].unsqueeze(0)\n","    else:\n","      input_batch['vis_feats'] = torch.cat((input_batch['vis_feats'], b['vis_feats'].unsqueeze(0)))\n","\n","    if input_batch['boxes'] is None:\n","      input_batch['boxes'] = b['boxes'].unsqueeze(0)\n","    else:\n","      input_batch['boxes'] = torch.cat((input_batch['boxes'], b['boxes'].unsqueeze(0)))\n","\n","    input_batch['input_ids'].append(b['input_ids'] + [0]*(max_input-len(b['input_ids'])))\n","    input_batch['target_ids'].append(b['target_ids'] + [0]*(max_target-len(b['target_ids'])))\n","    input_batch['scores'].append(b['scores'])\n","    input_batch['question_ids'].append(b['question_ids'])\n","\n","  input_batch['vis_feats'][input_batch['vis_feats'] == float(\"Inf\")] = 0\n","  input_batch['boxes'][input_batch['boxes'] == float(\"Inf\")] = 0\n","  input_batch['boxes'][input_batch['boxes'] == float(\"-Inf\")] = 0\n","  input_batch['input_ids'] = torch.tensor(input_batch['input_ids'])\n","  input_batch['target_ids'] = torch.tensor(input_batch['target_ids'])\n","  input_batch['scores'] = torch.tensor(input_batch['scores'])\n","  word_mask = input_batch['target_ids'] != 0\n","  input_batch['target_ids'][~word_mask] = -100\n","  return input_batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byC6Kv4IjeNI"},"outputs":[],"source":["def create_df(split):\n","  pairs = json.load(open(f'/content/OpenCQA/etc/data/{split}.json'))\n","  question_ids = []\n","  queries = []\n","  answers = []\n","  graph_bases = []\n","  graph_sem = []\n","  text_paths = []\n","  feature_paths = []\n","  input_texts = []\n","  for uid, pair in pairs.items():\n","    question_ids.append(uid)\n","    queries.append(pair[3])\n","    answers.append(str(pair[-2]))\n","    graph_bases.append(f'/content/OpenCQA_Graph/{uid}.pkl')\n","    graph_sem.append(f'/content/OpenCQA_Graph_6_rels/{uid}.pkl')\n","    text_paths.append(f'/content/text_feature_bert/{uid}.pkl')\n","    feature_paths.append(f'/content/mask-rcnn-predict_pkl/{uid}.pkl')\n","    ocr_data = json.load(open(f'/content/OpenCQA/bboxes/{uid}.json'))\n","    ocr_text = '|'.join([s['sentence'] for s in ocr_data])\n","    input_texts.append(pair[3] + ' <SEP> ' + pair[1] + ' <SEP> ' + ocr_text + '<SEP>')\n","  return pd.DataFrame({'queries': queries, 'answers': answers, 'question_ids': question_ids, 'graph_base': graph_bases, 'graph_sem': graph_sem, 'feature': feature_paths, 'input_texts': input_texts, 'text': text_paths})"]},{"cell_type":"markdown","metadata":{"id":"CF9kQHM4BSS4"},"source":["##Evaluator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIW3ChzyKa55"},"outputs":[],"source":["from nltk.translate.bleu_score import sentence_bleu\n","from nltk.tokenize import word_tokenize\n","from sacrebleu.metrics import BLEU, CHRF, TER\n","from sacremoses import MosesPunctNormalizer, MosesTokenizer, MosesDetokenizer\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jDndI2noWX0I"},"outputs":[],"source":["import csv\n","import json\n","from statistics import mean, stdev\n","import sys\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ImdrANwmfAN8"},"outputs":[],"source":["class VQAEvaluator:\n","    def __init__(self, df):\n","        # Loading datasets to data\n","        '''instances = pd.read_csv(src_folder + \"data.csv\")\n","        self.instances = instances\n","        self.inputs = instances[\"Input\"].values\n","        self.outputs = None\n","        if \"Output\" in instances:\n","            self.outputs = instances[\"Output\"].values\n","        self.images_indices = instances['Image Index'].values\n","        self.questions_ids = instances['Question ID'].values\n","        self.src_folder = src_folder'''\n","\n","        self.qidtoans = {}\n","        # Iterate through rows using iterrows()\n","        for index, row in df.iterrows():\n","            self.qidtoans[row['question_ids']] = row['answers']\n","        \"\"\"https://github.com/GT-Vision-Lab/VQA/blob/master/PythonEvaluationTools/vqaEvaluation/vqaEval.py\"\"\"\n","\n","        self.contractions = {\"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\": \"could've\", \"couldnt\": \"couldn't\", \\\n","\t\t\t\t\t\t\t \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\": \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\", \\\n","\t\t\t\t\t\t\t \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\": \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\", \\\n","\t\t\t\t\t\t\t \"he'dve\": \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\", \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", \\\n","\t\t\t\t\t\t\t \"Im\": \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\": \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\", \\\n","\t\t\t\t\t\t\t \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\": \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\", \\\n","\t\t\t\t\t\t\t \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\", \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\", \\\n","\t\t\t\t\t\t\t \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\", \\\n","\t\t\t\t\t\t\t \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\": \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\", \\\n","\t\t\t\t\t\t\t \"somebody'd\": \"somebodyd\", \"somebodyd've\": \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\": \"somebody'll\", \\\n","\t\t\t\t\t\t\t \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\", \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\", \\\n","\t\t\t\t\t\t\t \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\": \"something'd\", \"somethingd've\": \"something'd've\", \\\n","\t\t\t\t\t\t\t \"something'dve\": \"something'd've\", \"somethingll\": \"something'll\", \"thats\": \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\", \\\n","\t\t\t\t\t\t\t \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\": \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\", \\\n","\t\t\t\t\t\t\t \"they'dve\": \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\": \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\", \\\n","\t\t\t\t\t\t\t \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\": \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\", \\\n","\t\t\t\t\t\t\t \"whats\": \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\": \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\", \\\n","\t\t\t\t\t\t\t \"whod\": \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\": \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\", \\\n","\t\t\t\t\t\t\t \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\": \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\", \\\n","\t\t\t\t\t\t\t \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\", \\\n","\t\t\t\t\t\t\t \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\": \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", \\\n","\t\t\t\t\t\t\t \"youll\": \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"}\n","\n","        self.manualMap    = { 'none': '0',\n","\t\t\t\t\t\t\t  'zero': '0',\n","\t\t\t\t\t\t\t  'one': '1',\n","\t\t\t\t\t\t\t  'two': '2',\n","\t\t\t\t\t\t\t  'three': '3',\n","\t\t\t\t\t\t\t  'four': '4',\n","\t\t\t\t\t\t\t  'five': '5',\n","\t\t\t\t\t\t\t  'six': '6',\n","\t\t\t\t\t\t\t  'seven': '7',\n","\t\t\t\t\t\t\t  'eight': '8',\n","\t\t\t\t\t\t\t  'nine': '9',\n","\t\t\t\t\t\t\t  'ten': '10'\n","\t\t\t\t\t\t\t}\n","\n","        self.articles     = ['a',\n","\t\t\t\t\t\t\t 'an',\n","\t\t\t\t\t\t\t 'the'\n","\t\t\t\t\t\t\t]\n","\n","        self.periodStrip  = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n","        self.commaStrip   = re.compile(\"(\\d)(\\,)(\\d)\")\n","        self.punct        = [';', r\"/\", '[', ']', '\"', '{', '}',\n","\t\t\t\t\t\t\t '(', ')', '=', '+', '\\\\', '_', '-',\n","\t\t\t\t\t\t\t '>', '<', '@', '`', ',', '?', '!']\n","\n","        self.n = 2\n","\n","    def dump_result(self, quesid2ans: dict, path):\n","        \"\"\"\n","        Dump results to a json file, which could be submitted to the VQA online evaluation.\n","        VQA json file submission requirement:\n","            results = [result]\n","            result = {\n","                \"question_id\": int,\n","                \"answer\": str\n","            }\n","        :param quesid2ans: dict of quesid --> ans\n","        :param path: The desired path of saved file.\n","        \"\"\"\n","        with open(path, 'w') as f:\n","            result = []\n","            for ques_id, ans in quesid2ans.items():\n","                result.append({\n","                    'question_id': ques_id,\n","                    'answer': ans\n","                })\n","            json.dump(result, f, indent=4, sort_keys=True)\n","\n","    def evaluate_raw(self, quesid2ans: dict, is_topk_optimal=None, criteria='bleu'):\n","        \"\"\"https://github.com/GT-Vision-Lab/VQA/blob/master/PythonEvaluationTools/vqaEvaluation/vqaEval.py\"\"\"\n","\n","        # gts = self.dataset.id2datum_gt\n","\n","        self.accuracy     = {}\n","        self.evalQA       = {}\n","        self.evalQuesType = {}\n","        self.evalAnsType  = {}\n","\n","        accQA = []\n","        accQuesType = {}\n","        accAnsType = {}\n","\n","        # print(\"Computing accuracy\")\n","\n","        if criteria == 'bleu':\n","            mpn = MosesPunctNormalizer()\n","            mt = MosesTokenizer(lang=\"en\")\n","            md = MosesDetokenizer(lang=\"en\")\n","\n","            model_output_summary = []\n","            for quesId, resAns in tqdm(quesid2ans.items(), total=len(quesid2ans), ncols=80):\n","                model_output_summary.append(self.normalize_answer(resAns))\n","\n","            test_summary = list(self.qidtoans.values())\n","\n","            def detokenize(sent):\n","                sent = mpn.normalize(sent)\n","                tokens = mt.tokenize(sent)\n","                return md.detokenize(tokens)\n","\n","            model_output_summary = list(map(detokenize, model_output_summary))\n","            test_summary = list(map(detokenize, test_summary))\n","\n","            bleu = BLEU()\n","            bleuscore = bleu.corpus_score(model_output_summary, [test_summary]).score\n","\n","            self.setAccuracy(bleuscore)\n","\n","            return self.accuracy, model_output_summary\n","        elif criteria == 'cs':\n","            fillers = ['in', 'the', 'and', 'or', 'an', 'as', 'can', 'be', 'a', ':', '-',\n","           'to', 'but', 'is', 'of', 'it', 'on', '.', 'at', '(', ')', ',', ';']\n","\n","            count = 0\n","\n","            generatedScores = []\n","            #baselineScores = []\n","            untemplatedScores = [1,1]\n","\n","            gen_file = []\n","            for quesId, resAns in tqdm(quesid2ans.items(), total=len(quesid2ans), ncols=80):\n","                gen_file.append(self.normalize_answer(resAns))\n","\n","\n","\n","            with open('/content/testData.txt', 'r', encoding='utf-8') as dataFile, open('/content/testTitles.txt', 'r', encoding='utf-8') as titleFile, \\\n","                    open('/content/targetAnswers.txt', 'r', encoding='utf-8') as goldFile:\n","                for datas, titles, gold in zip(dataFile.readlines(), titleFile.readlines(), goldFile.readlines()):\n","                    dataArr = datas.split()\n","                    titleArr = titles.split()\n","                    goldArr = gold.split()\n","                    recordList = []\n","                    for gld in goldArr:\n","                        data_string = datas.replace(\"_\", \" \")\n","                        if gld.lower() in \" \".join([data_string,titles]).lower()  and gld.lower() not in fillers and gld.lower() not in recordList:\n","                            recordList.append(gld.lower())\n","                    list1 = recordList\n","                    list2 = recordList\n","                    list3 = recordList\n","                    recordLength = len(recordList)\n","                    generatedList = []\n","                    summary1 = gen_file[count]\n","\n","\n","                    for token in summary1.split():\n","                        if token.lower() in list1:\n","                            list1.remove(token.lower())\n","                            generatedList.append(token.lower())\n","\n","\n","                    count += 1\n","\n","                    if recordLength==0:\n","                        generatedRatio=0\n","                    else:\n","                        generatedRatio = len(generatedList) / recordLength\n","\n","\n","                    generatedScores.append(generatedRatio)\n","\n","            self.setAccuracy(mean(generatedScores)*100)\n","            return self.accuracy, None\n","\n","    def normalize_answer(self, resAns):\n","        resAns      = resAns.replace('<pad>', ' ')\n","        resAns      = resAns.replace('</s>', ' ')\n","        #resAns      = resAns.replace('\\n', ' ')\n","        #resAns      = resAns.replace('\\t', ' ')\n","        resAns      = resAns.strip()\n","        #resAns      = self.processPunctuation(resAns)\n","        #resAns      = self.processDigitArticle(resAns)\n","        #resAns = resAns.replace(',', '')\n","        return resAns\n","\n","    def processPunctuation(self, inText):\n","        outText = inText\n","        for p in self.punct:\n","            if (p + ' ' in inText or ' ' + p in inText) or (re.search(self.commaStrip, inText) != None):\n","                outText = outText.replace(p, '')\n","            else:\n","                outText = outText.replace(p, ' ')\n","        outText = self.periodStrip.sub(\"\",\n","                                        outText,\n","                                        re.UNICODE)\n","        return outText\n","\n","    def processDigitArticle(self, inText):\n","        outText = []\n","        tempText = inText.lower().split()\n","        for word in tempText:\n","            word = self.manualMap.setdefault(word, word)\n","            if word not in self.articles:\n","                outText.append(word)\n","            else:\n","                pass\n","        for wordId, word in enumerate(outText):\n","            if word in self.contractions:\n","                outText[wordId] = self.contractions[word]\n","        outText = ' '.join(outText)\n","        return outText\n","\n","    def setEvalQA(self, quesId, acc):\n","        self.evalQA[quesId] = round(100*acc, self.n)\n","\n","    def setEvalQuesType(self, quesId, quesType, acc):\n","        if quesType not in self.evalQuesType:\n","            self.evalQuesType[quesType] = {}\n","        self.evalQuesType[quesType][quesId] = round(100*acc, self.n)\n","\n","    def setEvalAnsType(self, quesId, ansType, acc):\n","        if ansType not in self.evalAnsType:\n","            self.evalAnsType[ansType] = {}\n","        self.evalAnsType[ansType][quesId] = round(100*acc, self.n)\n","\n","    def setAccuracy(self, bleuscore):\n","        self.accuracy['overall'] = bleuscore\n","        # self.accuracy['perQuestionType'] = {quesType: round(100*float(sum(accQuesType[quesType]))/len(accQuesType[quesType]), self.n) for quesType in accQuesType}\n","        # self.accuracy['perAnswerType']   = {ansType:  round(100*float(sum(accAnsType[ansType]))/len(accAnsType[ansType]), self.n) for ansType in accAnsType}\n","\n","    def within_percent(self, predicted, golden, tolerance=0.05):\n","      # Calculate the acceptable range\n","      tolerance = golden * tolerance\n","\n","      # Check if the predicted value is within the acceptable range\n","      if golden - tolerance <= predicted <= golden + tolerance:\n","          return True\n","      else:\n","          return False\n","\n","    def relaxed_correctness(self, target: str,\n","                        prediction: str,\n","                        max_relative_change: float = 0.05) -> bool:\n","      \"\"\"Calculates relaxed correctness.\n","\n","      The correctness tolerates certain error ratio defined by max_relative_change.\n","      See https://arxiv.org/pdf/2203.10244.pdf, end of section 5.1:\n","      Following Methani et al. (2020), we use a relaxed accuracy measure for the\n","      numeric answers to allow a minor inaccuracy that may result from the automatic\n","      data extraction process. We consider an answer to be correct if it is within\n","      5% of the gold answer. For non-numeric answers, we still need an exact match\n","      to consider an answer to be correct.\n","\n","      Args:\n","        target: Target string.\n","        prediction: Predicted string.\n","        max_relative_change: Maximum relative change.\n","\n","      Returns:\n","        Whether the prediction was correct given the specified tolerance.\n","      \"\"\"\n","      prediction_float = self._to_float(prediction)\n","      target_float = self._to_float(target)\n","      if prediction_float is not None and target_float:\n","          relative_change = abs(prediction_float -\n","                                target_float) / abs(target_float)\n","          return relative_change <= max_relative_change\n","      else:\n","          return prediction.lower() == target.lower()\n","\n","    def _to_float(self, text: str):\n","      try:\n","          if text.endswith('%'):\n","              # Convert percentages to floats.\n","              return float(text.rstrip('%')) / 100.0\n","          else:\n","              return float(text)\n","      except ValueError:\n","          return None"]},{"cell_type":"markdown","metadata":{"id":"bZPe31oaKbf3"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HYmvl4nAgmsd"},"outputs":[],"source":["def load_state_dict(state_dict_path, loc='cpu'):\n","    state_dict = torch.load(state_dict_path, map_location=loc)\n","    # Change Multi GPU to single GPU\n","    original_keys = list(state_dict.keys())\n","    for key in original_keys:\n","        if key.startswith(\"module.\"):\n","            new_key = key[len(\"module.\"):]\n","            state_dict[new_key] = state_dict.pop(key)\n","    return state_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NydbOsfHBrXn"},"outputs":[],"source":["load_checkpoint(model, '/content/drive/MyDrive/VL-T5/snap/pretrain/VLT5/Epoch30.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPDg3RuaOxgA"},"outputs":[],"source":["model.resize_token_embeddings(len(vlt5_tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQCDSxmCjkmM"},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLzBOWGjkKD9"},"outputs":[],"source":["df = create_df('train')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzW_78OXIKGL"},"outputs":[],"source":["train_dataset = GraphDataset(df, 36, vlt5_tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6uQ5FV6YmusE"},"outputs":[],"source":["train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=12, collate_fn=collator, num_workers=12)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BhTNpE3xH2RM"},"outputs":[],"source":["val_df = create_df('val')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DN4iZvhlIgew"},"outputs":[],"source":["val_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rHvI2_cbIN5B"},"outputs":[],"source":["val_dataset = GraphDataset(val_df, 36, vlt5_tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L4AEo5waIN5C"},"outputs":[],"source":["val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=12, collate_fn=collator, num_workers=12)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uRYZdYW5R77-"},"outputs":[],"source":["optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Id53voFR77-"},"outputs":[],"source":["model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QSmCabFwRSqs"},"outputs":[],"source":["torch.nn.init.xavier_uniform(model.proj.weight)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lryMGxB2j2gN"},"outputs":[],"source":["evaluator = VQAEvaluator(val_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GNqqGX0m0NOY"},"outputs":[],"source":["import subprocess\n","max_acc = 0\n","save_folder = '/content/drive/MyDrive/PL-NL/QA/OpenCQA/models/vl_t5_after_encoder_lr5_2layers'\n","create_folder_if_not_exists(save_folder)\n","save_losses = []\n","save_acc = []\n","from tqdm.autonotebook import tqdm\n","for epoch in range(1, 101):\n","  model.train()\n","  total_loss = 0\n","  with tqdm(range(len(train_dataloader))) as pbar:\n","    for bidx, batch in enumerate(train_dataloader):\n","        batch.pop('question_ids')\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        result = model.train_step(batch)\n","        loss = result['loss']\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        pbar.update(1)\n","        total_loss += loss.detach().item()\n","  total_loss = total_loss / len(train_dataloader)\n","  save_losses.append(total_loss)\n","  print(f'epoch {epoch} loss: {total_loss}')\n","  if epoch % 10 == 0:\n","      model.eval()\n","\n","      acc, anses = evaluate(model, val_dataloader, evaluator)\n","      acc = acc['overall']\n","      save_acc.append(acc)\n","\n","      print(f'Epoch {epoch} BLEU: {acc}')\n","      if acc > max_acc:\n","        max_acc = acc\n","        torch.save({\n","              'epoch': epoch,\n","              'model_state_dict': model.state_dict(),\n","              'optimizer_state_dict': optimizer.state_dict(),\n","              'acc': acc,\n","              }, f'{save_folder}/best.pt')\n","\n","  if (epoch) % 10 == 0:\n","    torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'loss': save_losses,\n","            'acc': save_acc,\n","            }, f'{save_folder}/model_{epoch}.pt')"]},{"cell_type":"markdown","metadata":{"id":"pMxlD908QkRC"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-5LNSdS_goBW"},"outputs":[],"source":["def load_state_dict(state_dict_path, loc='cpu'):\n","    state_dict = torch.load(state_dict_path, map_location=loc)['model_state_dict']\n","    # Change Multi GPU to single GPU\n","    original_keys = list(state_dict.keys())\n","    for key in original_keys:\n","        if key.startswith(\"module.\"):\n","            new_key = key[len(\"module.\"):]\n","            state_dict[new_key] = state_dict.pop(key)\n","    return state_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-nL64D7UwhPc"},"outputs":[],"source":["checkpoint = torch.load('/content/drive/MyDrive/PL-NL/QA/OpenCQA/models/vl_t5_after_encoder_lr5_2layers/best.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qm4ZJnXnfi4Z"},"outputs":[],"source":["model.resize_token_embeddings(len(vlt5_tokenizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5CtXnDIlwiTp"},"outputs":[],"source":["model.load_state_dict(checkpoint['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BELa9ZLLQ5Nt"},"outputs":[],"source":["test_df = create_df('test')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RystAkRFQ5N0"},"outputs":[],"source":["test_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A8UXz135Q5N0"},"outputs":[],"source":["test_dataset = GraphDataset(test_df, 36, vlt5_tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KKuRW4xQ5N0"},"outputs":[],"source":["test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=12, collate_fn=collator, num_workers=12)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVzVdsbFQ5N1"},"outputs":[],"source":["model = model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-DJfbKb_Q5N1"},"outputs":[],"source":["evaluator = VQAEvaluator(test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXSwjjUDQ5N1"},"outputs":[],"source":["max_acc = 0\n","from tqdm.autonotebook import tqdm\n","model.eval()\n","acc, results = evaluate(model, test_dataloader, evaluator)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmmLQkNSldmQ"},"outputs":[],"source":["acc"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1VA-MB-KwhTAfm0vzaFM6z7fL0nK942bR","timestamp":1725188415299},{"file_id":"1mfislW8OJ1-b2v37v9pkUmX5iJ-SIDGu","timestamp":1713505039133},{"file_id":"1G8cdL5eA4gOMfZWw5SnIoUweY-F1wc3a","timestamp":1711081689694},{"file_id":"1DQNk6x3fKR2_6QOUn4D7ZnHXie44L9Dm","timestamp":1710580357022},{"file_id":"1-MHAxvz3sSJlsR0dZKY4g7kWtu6sjjHW","timestamp":1710053909866},{"file_id":"1vMvRBhYqFdjS1IzVyLL1P7bQoTUhV3M-","timestamp":1709973713778},{"file_id":"1uZBb-c1O-Go9Z0E4-rakrig39YhiM6yO","timestamp":1708345170929},{"file_id":"1-C5UshCuYI78zwfqkUtauI3An1rs-sR8","timestamp":1708306188548},{"file_id":"1jwAIluxqSSLNgZalNuEQ6qvh_kjoj6hX","timestamp":1708057869258},{"file_id":"1n0ezN4C3IkRLamfZnBItWwnI2-18kL_b","timestamp":1707988494084},{"file_id":"13b6wK6tAAwP72PuiRFZ5OiUMGv1Tfzlw","timestamp":1707953867458},{"file_id":"11RCF0A0HAutbR7-XQdo0eJX0L6kHbZk3","timestamp":1707803664840},{"file_id":"11Nbs20r_F3cGxHMhZDe64QxQA2lNJ3fH","timestamp":1707788315374}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}